{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "project_dir = os.path.split(os.getcwd())[0]\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "ipdl_dir = os.path.join(project_dir, \"ipdl/\")\n",
    "if ipdl_dir not in sys.path:\n",
    "    sys.path.append(ipdl_dir)\n",
    "\n",
    "from IPAE import MatrixEstimator, SDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch\n",
    "from functools import reduce\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "from IPDL import TensorKernel\n",
    "\n",
    "class MatrixEstimator(nn.Module):\n",
    "    def __init__(self, sigma: float):\n",
    "        super(MatrixEstimator, self).__init__()\n",
    "        \n",
    "        self.sigma = sigma\n",
    "        self.x = torch.rand((10, 1))\n",
    "\n",
    "    def set_sigma(self, sigma: float) -> None:\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if not self.training:\n",
    "            self.x = x.detach().cpu()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_matrix(self) -> Tensor:\n",
    "        '''\n",
    "            Return matrix A\n",
    "        '''\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        n = self.x.size(0)\n",
    "        return (TensorKernel.RBF(self.x.flatten(1).to(device), self.sigma) / n).cpu()\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"MatrixEstimator(sigma={})\".format(self.sigma)\n",
    "\n",
    "\n",
    "\n",
    "matrix_estimator = MatrixEstimator(0.8)\n",
    "\n",
    "model = nn.Sequential(\n",
    "        # nn.Linear(10, 10),\n",
    "        nn.Identity(),\n",
    "        matrix_estimator,\n",
    "        nn.Linear(10, 2),\n",
    "        MatrixEstimator(0.8)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Identity()\n  (1): MatrixEstimator(sigma=0.8)\n  (2): Linear(in_features=10, out_features=2, bias=True)\n  (3): MatrixEstimator(sigma=0.8)\n)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.2078, 0.5078, 0.4116, 0.0355, 0.8846, 0.0701, 0.7639, 0.6777, 0.0431,\n         0.7622]])\ntensor([[0.8907],\n        [0.4822],\n        [0.2004],\n        [0.2244],\n        [0.8916],\n        [0.9460],\n        [0.8299],\n        [0.5061],\n        [0.6178],\n        [0.1399]])\ntensor([[0.2078, 0.5078, 0.4116, 0.0355, 0.8846, 0.0701, 0.7639, 0.6777, 0.0431,\n         0.7622]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 10)\n",
    "print(x)\n",
    "model.train()\n",
    "model(x)\n",
    "\n",
    "print(matrix_estimator.x)\n",
    "model.eval()\n",
    "model(x)\n",
    "print(matrix_estimator.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixOptimizer():\n",
    "    def __init__(self, model: nn.Module,  beta=0.5):\n",
    "        if not(0 <= beta <= 1):\n",
    "            raise Exception('beta must be in the range [0, 1]')\n",
    "\n",
    "        self.matrix_estimators = []\n",
    "        # First element corresponds to input A matrix and last element\n",
    "        # is the output A matrix\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, (MatrixEstimator)):\n",
    "                self.matrix_estimators.append(module)\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "        self.sigma_prev = [-1] * len(matrix_estimators)\n",
    "\n",
    "    # Temporal, just for testing\n",
    "    # def getSigmaValues(self):\n",
    "    #     return self.sigma_tmp\n",
    "\n",
    "    # def getSigma(self):\n",
    "    #     return self.sigma\n",
    "\n",
    "    '''\n",
    "        @param The output of a specific layer\n",
    "        @param label_kernel_matrix\n",
    "        @param n_sigmas\n",
    "    '''\n",
    "    def step(self, layer_output: Tensor, Ky: Tensor, sigma_values: list) -> float:\n",
    "        sigma_t = self.optimize(layer_output, Ky, sigma_values)\n",
    "        self.sigma = ( (self.beta*sigma_t) + ((1-self.beta)*self.sigma) ) if not(self.sigma is None) else sigma_t\n",
    "        return self.getSigma()\n",
    "\n",
    "    '''\n",
    "        This function is used in orter to obtain the optimal kernel width for\n",
    "        an T DNN layer\n",
    "\n",
    "        @param layer_output\n",
    "        @param n_sigmas: number of possible sigma values\n",
    "\n",
    "        [DescripciÃ³n del procedimiento]\n",
    "    '''\n",
    "    def optimize(self, x: Tensor, Ky: Tensor, sigma_values: list) -> float:\n",
    "        Kt = list( map(lambda sigma: TensorKernel.RBF(x, sigma).detach(), sigma_values) )\n",
    "        loss = np.array( list( map(lambda k: self.kernelAligmentLoss(k, Ky), Kt) ) )\n",
    "\n",
    "        self.sigma_tmp.append(sigma_values[ np.argwhere(loss == loss.max()).item(0) ])\n",
    "        return self.sigma_tmp[-1]\n",
    "\n",
    "    '''\n",
    "        Kernel Aligment Loss Function.\n",
    "\n",
    "        This function is used in order to obtain the optimal sigma parameter from\n",
    "        RBF kernel.  \n",
    "    '''\n",
    "    def kernelAligmentLoss(self, x: Tensor, y: Tensor) -> float:\n",
    "        return (torch.sum(x*y)/(torch.norm(x) * torch.norm(y))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "A = [0] * 10\n",
    "\n",
    "A[0] = 1\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dist_mat(self, x, y=None):\n",
    "#         try:\n",
    "#             x = th.from_numpy(x)\n",
    "#         except TypeError:\n",
    "#             x = x\n",
    "\n",
    "#         dist = th.norm(x[:, None] - x, dim=2, p=2)\n",
    "#         return dist / dist.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.0000,  2.8284,  5.6569,  8.4853, 11.3137],\n        [ 2.8284,  0.0000,  2.8284,  5.6569,  8.4853],\n        [ 5.6569,  2.8284,  0.0000,  2.8284,  5.6569],\n        [ 8.4853,  5.6569,  2.8284,  0.0000,  2.8284],\n        [11.3137,  8.4853,  5.6569,  2.8284,  0.0000]])\ntensor([[ 0.0000,  2.8284,  5.6569,  8.4853, 11.3137],\n        [ 2.8284,  0.0000,  2.8284,  5.6569,  8.4853],\n        [ 5.6569,  2.8284,  0.0000,  2.8284,  5.6569],\n        [ 8.4853,  5.6569,  2.8284,  0.0000,  2.8284],\n        [11.3137,  8.4853,  5.6569,  2.8284,  0.0000]])\ntensor([[ 0.0000,  2.8284,  5.6569,  8.4853, 11.3137],\n        [ 2.8284,  0.0000,  2.8284,  5.6569,  8.4853],\n        [ 5.6569,  2.8284,  0.0000,  2.8284,  5.6569],\n        [ 8.4853,  5.6569,  2.8284,  0.0000,  2.8284],\n        [11.3137,  8.4853,  5.6569,  2.8284,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10, dtype=torch.float32).reshape((5,2))\n",
    "b = torch.arange(5, 15)\n",
    "\n",
    "dist = torch.norm(a[:, None] - a, dim=2, p=2)\n",
    "print(dist)\n",
    "\n",
    "pairwise_difference = (torch.unsqueeze(a,1) - torch.unsqueeze(a,0))\n",
    "distance = torch.sqrt(torch.sum(pairwise_difference**2, dim=2))\n",
    "print(distance)\n",
    "distance = torch.cdist(a,a, p=2)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Invalid norm order 'fro' for vectors",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-96bbb937388d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/Workspace/Anaconda/Miniconda/envs/DeepLearning/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# are valid for vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2565\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid norm order '{ord}' for vectors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2566\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2567\u001b[0m             \u001b[0mabsx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid norm order 'fro' for vectors"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "np_test = a.numpy()\n",
    "LA.norm(a, ord='fro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}